{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF ON COLAB\n",
    "############Ã SEARCH HOW TO SET ON COLAB PYTHON 3.5, IF NOT IT DOES NOT WORK\n",
    "!pip install gymnasium \"gymnasium[classic-control,mujoco]==0.29.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#CPU set-up with GPU available: run this:\n",
    "device=torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Else:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.Tensor(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.Tensor(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_Agent(): #this should work with more parallel environment already, not the evaluate functions\n",
    "\n",
    "    def __init__(self, n_steps, actor, critic, lr_actor, lr_critic, device):\n",
    "        self.n = n_steps\n",
    "        self.device = device\n",
    "        self.actor = actor.to(self.device) \n",
    "        self.critic = critic.to(self.device)\n",
    "        self.actor_optimizer = torch.optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = torch.optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "    \n",
    "\n",
    "    def choose_action(self, states, action_type = \"training\"):        \n",
    "        state_values = self.critic.forward(states)\n",
    "        action_logits = self.actor.forward(states)\n",
    "        \n",
    "\n",
    "        if action_type == \"training\":\n",
    "            actions_pd = torch.distributions.Categorical(logits=action_logits)\n",
    "            actions = actions_pd.sample()\n",
    "            actions_log_prog = actions_pd.log_prob(actions)\n",
    "            return actions, actions_log_prog, state_values\n",
    "        \n",
    "        elif action_type == \"greedy\":\n",
    "            actions = torch.argmax(action_logits, dim=1) #greedy policy\n",
    "            return actions, state_values\n",
    "    \n",
    "    def get_losses(self, rewards, states, next_states, log_prob, gamma, terminated):\n",
    "        \n",
    "        delta = torch.Tensor(rewards).reshape_as(self.critic(next_states)) + (1-torch.Tensor(terminated).reshape_as(self.critic(next_states))) * gamma * self.critic(next_states)\n",
    "        advantage = delta - self.critic(states)        \n",
    "\n",
    "        critic_loss = advantage.pow(2).mean() \n",
    "        \n",
    "\n",
    "        \n",
    "        actor_loss = - (advantage.detach() * log_prob.reshape_as(advantage)).mean() #minus sign ? not in slides\n",
    "        # actor_loss = -log_prob*advantage.detach()\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_params(self, critic_loss, actor_loss):\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def evaluate_performance(self, env, total_steps):\n",
    "        \n",
    "        episodic_rewards_10 = []\n",
    "        v_values = []\n",
    "        for j in range(10):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, state_values = self.choose_action(state, action_type= \"greedy\")\n",
    "                #get the current value function fro one trajectory (j==0)\n",
    "                if j ==0 : v_values.append( state_values.detach().data.numpy() ) #shitty with float\n",
    "                \n",
    "                #go to the next state\n",
    "                next_state, reward, terminated, truncated, _  = env.step(action.detach().data.numpy())\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                done = terminated or truncated\n",
    "            episodic_rewards_10.append(total_reward)\n",
    "        print(\"episodic return of the 20000:\", np.mean(episodic_rewards_10))\n",
    "        #comment this if you are trying with more envs\n",
    "        #plt.plot(v_values)\n",
    "        #plt.xlabel(\"trajectory\")\n",
    "        #plt.xlabel(\"V lalue\")\n",
    "        #plt.title(\"V values for the first evaluation trajectory\")\n",
    "        #plt.savefig(f'figures/v_values_{total_steps}.png')\n",
    "        #plt.close()\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    def evaluate_training(self, total_reward, actor_loss, critic_loss):\n",
    "        print(\"current episodic return:\", np.log(total_reward))\n",
    "        print(\"critic loss:\", np.log(critic_loss.detach().data.numpy()))\n",
    "        print(\"actor loss:\", np.log(actor_loss.detach().data.numpy()))\n",
    "        print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "total_steps = 0\n",
    "max_steps = 500000\n",
    "env = gym.vector.AsyncVectorEnv( [lambda:gym.make(\"CartPole-v1\") for i in range(1)] )\n",
    "state_dim  = env.single_observation_space.shape[0]\n",
    "n_actions = env.single_action_space.n\n",
    "actor = Actor(state_dim, n_actions)\n",
    "critic = Critic(state_dim)\n",
    "agent = A2C_Agent(n_steps=1, actor= actor, critic= critic, lr_actor=1e-5, lr_critic=1e-3, device= device)\n",
    "state = env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current episodic return: [0.]\n",
      "critic loss: 0.0154156815\n",
      "actor loss: -0.41033\n",
      "\n",
      "\n",
      "episodic return of the 20000: 9.2\n",
      "\n",
      "\n",
      "current episodic return: [2.77258872]\n",
      "critic loss: 0.23437867\n",
      "actor loss: -0.229189\n",
      "\n",
      "\n",
      "current episodic return: [3.40119738]\n",
      "critic loss: -0.30066654\n",
      "actor loss: -0.53412783\n",
      "\n",
      "\n",
      "current episodic return: [0.]\n",
      "critic loss: -0.30520827\n",
      "actor loss: -0.5280647\n",
      "\n",
      "\n",
      "current episodic return: [1.79175947]\n",
      "critic loss: -0.28957137\n",
      "actor loss: -0.314291\n",
      "\n",
      "\n",
      "current episodic return: [2.48490665]\n",
      "critic loss: -0.03052327\n",
      "actor loss: -0.32767397\n",
      "\n",
      "\n",
      "current episodic return: [3.29583687]\n",
      "critic loss: -0.39003402\n",
      "actor loss: -0.3645774\n",
      "\n",
      "\n",
      "current episodic return: [2.19722458]\n",
      "critic loss: -0.72488904\n",
      "actor loss: nan\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14244/455609580.py:84: RuntimeWarning: invalid value encountered in log\n",
      "  print(\"actor loss:\", np.log(actor_loss.detach().data.numpy()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current episodic return: [2.19722458]\n",
      "critic loss: -0.99219286\n",
      "actor loss: -1.2008941\n",
      "\n",
      "\n",
      "current episodic return: [3.17805383]\n",
      "critic loss: -0.38500783\n",
      "actor loss: -0.81013316\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     12\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 14\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m critic_losses\u001b[38;5;241m.\u001b[39mappend(critic_loss)\n\u001b[1;32m     19\u001b[0m actor_losses\u001b[38;5;241m.\u001b[39mappend(actor_loss)\n",
      "Cell \u001b[0;32mIn[21], line 50\u001b[0m, in \u001b[0;36mA2C_Agent.update_params\u001b[0;34m(self, critic_loss, actor_loss)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_project/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_project/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_project/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_project/lib/python3.9/site-packages/torch/optim/adam.py:80\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m state_values:\n\u001b[1;32m     78\u001b[0m             s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_group\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     group,\n\u001b[1;32m     83\u001b[0m     params_with_grad,\n\u001b[1;32m     84\u001b[0m     grads,\n\u001b[1;32m     85\u001b[0m     exp_avgs,\n\u001b[1;32m     86\u001b[0m     exp_avg_sqs,\n\u001b[1;32m     87\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m     88\u001b[0m     state_steps\n\u001b[1;32m     89\u001b[0m ):\n\u001b[1;32m     90\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_project/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while total_steps < max_steps:  \n",
    "    \n",
    "    terminated = False\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset() \n",
    "    while not terminated:\n",
    "        actions, actions_log_prog, state_values = agent.choose_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(actions.numpy())\n",
    "        actor_loss, critic_loss = agent.get_losses(reward, state, next_state, actions_log_prog, 0.99, terminated)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        agent.update_params(critic_loss, actor_loss)\n",
    "\n",
    "        \n",
    "\n",
    "        critic_losses.append(critic_loss)\n",
    "        actor_losses.append(actor_loss)\n",
    "\n",
    "\n",
    "        if (total_steps % 1000) == 0: agent.evaluate_training(total_reward, actor_loss, critic_loss)\n",
    "        if (total_steps % 20000 == 0):agent.evaluate_performance(env, total_steps)\n",
    "\n",
    "\n",
    "        terminated = terminated.all()\n",
    "        if total_steps >= max_steps:\n",
    "            break  \n",
    "        else: total_steps += 1\n",
    "        \n",
    "            \n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
