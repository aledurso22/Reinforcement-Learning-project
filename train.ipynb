{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(results, filename):\n",
    "    # Define the headers for your CSV file\n",
    "    headers = ['Mean episodic reward', 'Critic loss', 'Actor loss']  # Add your result names here\n",
    "\n",
    "    # Open the CSV file in 'append' mode so that existing data isn't overwritten\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers if the file is empty\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writerow(headers)\n",
    "        \n",
    "        # Write the results to the CSV file\n",
    "        writer.writerow(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF ON COLAB\n",
    "############Ã SEARCH HOW TO SET ON COLAB PYTHON 3.5, IF NOT IT DOES NOT WORK\n",
    "!pip install gymnasium \"gymnasium[classic-control,mujoco]==0.29.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#CPU set-up with GPU available: run this:\n",
    "device=torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Else:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,n_actions),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.Tensor(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.Tensor(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_Agent(): #this should work with more parallel environment already, not the evaluate functions\n",
    "\n",
    "    def __init__(self, n_envs, n_steps, actor, critic, lr_actor, lr_critic):\n",
    "        self.n_envs = n_envs\n",
    "        self.n = n_steps\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.actor_optimizer = torch.optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = torch.optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "    \n",
    "\n",
    "    def choose_action(self, states, action_type = \"training\"):        \n",
    "        state_values = self.critic.forward(states)\n",
    "        action_probs = self.actor.forward(states)\n",
    "        \n",
    "\n",
    "        if action_type == \"training\":\n",
    "            actions_pd = torch.distributions.Categorical(probs=action_probs)\n",
    "            actions = actions_pd.sample()\n",
    "            actions_log_prog = actions_pd.log_prob(actions)\n",
    "            return actions, actions_log_prog, state_values\n",
    "        \n",
    "        elif action_type == \"greedy\":\n",
    "            actions = torch.argmax(action_probs) #greedy policy\n",
    "            return actions, state_values\n",
    "    \n",
    "    def get_losses(self, rewards, states, next_states, log_prob, gamma, terminated):\n",
    "        \n",
    "        delta = torch.Tensor(rewards.reshape(self.n_envs,1)) + (1-torch.Tensor(terminated.reshape(self.n_envs,1))) * gamma * self.critic(next_states)\n",
    "        advantage = delta - self.critic(states)        \n",
    "\n",
    "        critic_loss = advantage.pow(2).sum()     \n",
    "        \n",
    "        actor_loss = - (advantage.detach() * log_prob.reshape_as(advantage.detach())).sum()\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_params(self, critic_loss, actor_loss):\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def evaluate_performance(self):\n",
    "        \n",
    "        episodic_rewards_10 = []\n",
    "        v_values = []\n",
    "        # env = gym.vector.AsyncVectorEnv( [lambda:gym.make(\"CartPole-v1\") for i in range(1)])\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        for j in range(10):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, state_values = self.choose_action(state, action_type= \"greedy\")\n",
    "                #get the current value function fro one trajectory (j==0)\n",
    "                if j ==0 : v_values.append( state_values.detach().data.numpy() ) #shitty with float\n",
    "                \n",
    "                #go to the next state\n",
    "                next_state, reward, terminated, truncated, _  = env.step(action.detach().data.numpy())\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                done = terminated or truncated\n",
    "            episodic_rewards_10.append(total_reward)\n",
    "        print(\"episodic return of the 20000:\", np.mean(episodic_rewards_10))\n",
    "        #comment this if you are trying with more envs\n",
    "        #plt.plot(v_values)\n",
    "        #plt.xlabel(\"trajectory\")\n",
    "        #plt.xlabel(\"V lalue\")\n",
    "        #plt.title(\"V values for the first evaluation trajectory\")\n",
    "        #plt.savefig(f'figures/v_values_{total_steps}.png')\n",
    "        #plt.close()\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    def evaluate_training(self, episode_rewards, actor_loss, critic_loss, filename):\n",
    "        # print(\"current episodic return:\", np.mean(episode_rewards))\n",
    "        # print(\"critic loss:\", (critic_loss.detach().data.numpy()))\n",
    "        # print(\"actor loss:\", (actor_loss.detach().data.numpy()))\n",
    "        log_results([np.mean(episode_rewards), critic_loss.detach().data.numpy(), actor_loss.detach().data.numpy()], filename)\n",
    "        # print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "total_steps = 0\n",
    "max_steps = 500000\n",
    "env = gym.vector.AsyncVectorEnv( [lambda:gym.make(\"CartPole-v1\") for i in range(1)] )\n",
    "state_dim  = env.single_observation_space.shape[0]\n",
    "n_actions = env.single_action_space.n\n",
    "actor = Actor(state_dim, n_actions)\n",
    "critic = Critic(state_dim)\n",
    "agent = A2C_Agent(n_envs=1, n_steps=1, actor= actor, critic= critic, lr_actor=1e-5, lr_critic=1e-3)\n",
    "state = env.reset()[0]\n",
    "filename = '1_' + '1k_res.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0034],\n",
      "        [1.0033]], grad_fn=<SubBackward0>) tensor([-0.6596, -0.6618], grad_fn=<SqueezeBackward1>)\n",
      "[[ 0.03764677 -0.02733363  0.03910925  0.03694802]\n",
      " [-0.02120274  0.01488612  0.04249864  0.03370323]] tensor([1, 1]) tensor([-0.6596, -0.6618], grad_fn=<SqueezeBackward1>) tensor([[0.1526],\n",
      "        [0.1585]], grad_fn=<AddmmBackward0>)\n",
      "[[ 0.0371001   0.1672063   0.03984821 -0.24314356]\n",
      " [-0.02090501  0.20937368  0.04317271 -0.24527384]] [1. 1.] [False False] [False False]\n",
      "tensor(1.3259, grad_fn=<NegBackward0>) tensor(2.0134, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "actions, actions_log_prog, state_values = agent.choose_action(state)\n",
    "next_state, reward, terminated, truncated, _ = env.step(actions.numpy())\n",
    "actor_loss, critic_loss = agent.get_losses(reward, state, next_state, actions_log_prog, 0.99, terminated)\n",
    "print(state, actions, actions_log_prog, state_values)\n",
    "print(next_state, reward, terminated, truncated)\n",
    "print(actor_loss, critic_loss)\n",
    "# print(critic(state))\n",
    "# print(torch.Tensor(reward.reshape(2,1)) + 0.99 * critic(next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0122,  0.0330, -0.0165, -0.0243],\n",
       "        [ 0.0274,  0.0239,  0.0124, -0.0317]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasbrunet/opt/anaconda3/envs/rl_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/Users/thomasbrunet/opt/anaconda3/envs/rl_project/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/thomasbrunet/opt/anaconda3/envs/rl_project/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodic return of the 20000: 9.5\n",
      "\n",
      "\n",
      "episodic return of the 20000: 87.7\n",
      "\n",
      "\n",
      "episodic return of the 20000: 500.0\n",
      "\n",
      "\n",
      "episodic return of the 20000: 399.9\n",
      "\n",
      "\n",
      "episodic return of the 20000: 420.8\n",
      "\n",
      "\n",
      "episodic return of the 20000: 490.0\n",
      "\n",
      "\n",
      "episodic return of the 20000: 299.7\n",
      "\n",
      "\n",
      "episodic return of the 20000: 165.1\n",
      "\n",
      "\n",
      "episodic return of the 20000: 229.5\n",
      "\n",
      "\n",
      "episodic return of the 20000: 194.6\n",
      "\n",
      "\n",
      "episodic return of the 20000: 241.1\n",
      "\n",
      "\n",
      "episodic return of the 20000: 235.8\n",
      "\n",
      "\n",
      "episodic return of the 20000: 114.4\n",
      "\n",
      "\n",
      "episodic return of the 20000: 127.6\n",
      "\n",
      "\n",
      "episodic return of the 20000: 129.3\n",
      "\n",
      "\n",
      "episodic return of the 20000: 146.0\n",
      "\n",
      "\n",
      "episodic return of the 20000: 193.0\n",
      "\n",
      "\n",
      "episodic return of the 20000: 183.4\n",
      "\n",
      "\n",
      "episodic return of the 20000: 136.4\n",
      "\n",
      "\n",
      "episodic return of the 20000: 478.1\n",
      "\n",
      "\n",
      "episodic return of the 20000: 500.0\n",
      "\n",
      "\n",
      "episodic return of the 20000: 466.6\n",
      "\n",
      "\n",
      "episodic return of the 20000: 489.4\n",
      "\n",
      "\n",
      "episodic return of the 20000: 241.4\n",
      "\n",
      "\n",
      "episodic return of the 20000: 500.0\n",
      "\n",
      "\n",
      "episodic return of the 20000: 500.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while total_steps < max_steps:  \n",
    "\n",
    "    terminated = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    \n",
    "    while not terminated:\n",
    "        actions, actions_log_prog, state_values = agent.choose_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(actions.numpy())\n",
    "        actor_loss, critic_loss = agent.get_losses(reward, state, next_state, actions_log_prog, 0.99, terminated)\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        agent.update_params(critic_loss, actor_loss)\n",
    "\n",
    "        if (total_steps % 1000) == 0: \n",
    "            agent.evaluate_training(episode_rewards, actor_loss, critic_loss, filename)\n",
    "            episode_rewards = []\n",
    "        if (total_steps % 20000 == 0): agent.evaluate_performance()\n",
    "\n",
    "\n",
    "        terminated = terminated or truncated\n",
    "\n",
    "        if total_steps >= max_steps:\n",
    "            break  \n",
    "        else: total_steps += 1\n",
    "        \n",
    "            \n",
    "    episode_rewards.append(ep_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
